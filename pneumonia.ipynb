{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZBCbqFNqi4Zm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip '/content/drive/MyDrive/Pneumonia Pytorch/archive.zip' -d '/content'"
      ],
      "metadata": {
        "id": "SkfrCFpEiyzR"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "lllCpo0vSFB_"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import make_grid\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms, datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir = \"/content/chest_xray/chest_xray\"\n"
      ],
      "metadata": {
        "id": "NpbGcWYPTxQt"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define transforms and dataset\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(\n",
        "    os.path.join(dir, 'train/'), transform=transform_train)\n",
        "\n",
        "val_dataset = datasets.ImageFolder(\n",
        "    os.path.join(dir, 'val/'), transform=transform_val)\n",
        "\n",
        "test_dataset = datasets.ImageFolder(\n",
        "    os.path.join(dir, 'test/'), transform=transform_val)"
      ],
      "metadata": {
        "id": "L4WEGxU6bhKh"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define data loaderes\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size = 64, shuffle=True)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size = 64, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset, batch_size = 64, shuffle=True)"
      ],
      "metadata": {
        "id": "BFvMfIzwmBXi"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, gts = next(iter(train_loader))\n",
        "print(images.shape)\n",
        "print(gts.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Z2tNxIJrBgG",
        "outputId": "126ad909-08d8-4004-ee9d-5b8dbda02cd3"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 3, 224, 224])\n",
            "torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = torchvision.models.densenet121(pretrained=True).to(device)\n",
        "\n",
        "features = model.classifier.in_features\n",
        "model.classifier = torch.nn.Linear(features, 2).to(device)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VTib8eo8u9s",
        "outputId": "a84e9a73-bf4a-427e-9df0-92eb893251a4"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = 5\n",
        "\n",
        "best_loss = None\n",
        "\n",
        "for i in range(epoch):\n",
        "  loss = 0\n",
        "\n",
        "  # training loop\n",
        "  for inputs,gts in train_loader:\n",
        "    model.train()\n",
        "    inputs = inputs.to(device)\n",
        "    gts = gts.to(device)\n",
        "    optimizer.zero_grad() \n",
        "    out = model(inputs)\n",
        "    loss = criterion(out, gts)\n",
        "    loss.backward() \n",
        "    optimizer.step() \n",
        "\n",
        "    loss += loss.item()\n",
        "\n",
        "  # save best loss checkpoint\n",
        "  if best_loss is None:\n",
        "    torch.save(model.state_dict(), f'/content/drive/MyDrive/pneumonia.pt') \n",
        "    print('Saving Model')\n",
        "    best_loss = loss\n",
        "  elif best_loss > loss:\n",
        "    torch.save(model.state_dict(), f'/content/drive/MyDrive/pneumonia.pt') \n",
        "    print('Saving Model')\n",
        "    best_loss = loss\n",
        "\n",
        "  print('Epoch ' + str(i+1) + ', Loss: ' + str(loss.item()))"
      ],
      "metadata": {
        "id": "9_VcfTNg_zWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load best model and run it on test set\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/pneumonia.pt'))    \n",
        "all = 0\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "  for inputs , gts in test_loader:\n",
        "      model.eval()\n",
        "      inputs = inputs.to(device)\n",
        "      gts = gts.to(device)\n",
        "      out = model(inputs)\n",
        "      pred = torch.max(out, 1)[1]\n",
        "      correct += pred.eq(gts.data.view_as(pred)).sum()\n",
        "\n",
        "\n",
        "print(\"Total number of Test Images : \" + str(len(test_loader.dataset)))\n",
        "print(\"Accuracy : \" + str((correct/len(test_dataset) * 100).item()) )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlN2s44LJS77",
        "outputId": "44b02962-998b-4457-d439-110d50e125aa"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of Test Images : 624\n",
            "Accuracy : 91.9871826171875\n"
          ]
        }
      ]
    }
  ]
}